{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# langsung dari library efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all train images + labels according to folders -> df\n",
    "import pandas as pd\n",
    "from fastai.vision import *\n",
    "\n",
    "def get_data(bs, size): \n",
    "    tfms = get_transforms(max_lighting=0.4, max_zoom=1.2, max_warp=0.2, max_rotate=20, xtra_tfms=[flip_lr()])\n",
    "    return ImageDataBunch.from_folder(Path('./dataset'),\n",
    "                                  train = 'train/',\n",
    "                                  valid_pct = 0.1,\n",
    "                                  resize_method=ResizeMethod.SQUISH, \n",
    "                                  ds_tfms = tfms,\n",
    "                                  size = size,\n",
    "                                  bs = bs,\n",
    "                                  num_workers = 50\n",
    "                                  ).normalize(imagenet_stats)\n",
    "\n",
    "data = get_data(32, 299)\n",
    "img_path = [str(x) for x in list(data.train_ds.items)] + list(data.valid_ds.items)\n",
    "labels = [data.classes[x] for x in list(data.train_ds.y.items) + list(data.valid_ds.y.items)]\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch \n",
    "from torchvision import transforms\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "\n",
    "\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "tfms = transforms.Compose([transforms.Resize(224), transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),]) # pake punya imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(img):\n",
    "    img = tfms(Image.open(img)).unsqueeze(0)\n",
    "    f = model.extract_features(img)\n",
    "    return f[0].detach().cpu().numpy()\n",
    "\n",
    "img_vectors = [get_embeddings(x) for x in tqdm(img_path)]\n",
    "\n",
    "np.save('training_efnet_vectors_229', img_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tfms(Image.open('./test/56d6e2b4ccdaf0fc97f24d5aba7ad672.jpg')).unsqueeze(0)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = model.extract_features(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1280, 7, 7])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape # get all 1280 dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_vec = features[0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 7, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1280"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15575688, -0.18157749, -0.2616584 , -0.27844825, -0.01997108,\n",
       "       -0.02957596, -0.23550095], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_vec[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
